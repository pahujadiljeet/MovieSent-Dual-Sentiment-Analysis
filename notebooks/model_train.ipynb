{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Correct path do (IMDB dataset ko apni drive main rakho)\n",
        "import pandas as pd\n",
        "\n",
        "# yahan tum apna actual path daalo, example:\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/IMDB Dataset.csv\")\n",
        "\n",
        "# Step 3: Check data\n",
        "print(df.head())\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Step 4: Handle missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Step 5: Split data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df['review']\n",
        "y = df['sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: TF-IDF vectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
        "X_test_tfidf = tfidf.transform(X_test).toarray()\n",
        "\n",
        "# Step 7: Logistic Regression model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "log_model = LogisticRegression(max_iter=200)\n",
        "log_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Step 8: Accuracy check\n",
        "y_pred = log_model.predict(X_test_tfidf)\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Step 9: Save model + vectorizer\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Drive me folder ka path (apna folder path change kar sakte ho)\n",
        "drive_model_path = \"/content/drive/MyDrive/MovieSentModels\"\n",
        "os.makedirs(drive_model_path, exist_ok=True)\n",
        "\n",
        "# Save models aur vectorizer\n",
        "joblib.dump(log_model, os.path.join(drive_model_path, \"log_model.pkl\"))\n",
        "joblib.dump(tfidf, os.path.join(drive_model_path, \"tfidf.pkl\"))\n",
        "joblib.dump(X_train, os.path.join(drive_model_path, \"X_train.pkl\"))\n",
        "joblib.dump(y_train, os.path.join(drive_model_path, \"y_train.pkl\"))\n",
        "\n",
        "print(f\"✅ Models aur vectorizer successfully save ho gaye Google Drive me: {drive_model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_DhqJr9wg5H",
        "outputId": "41d4c018-198d-4648-b0ca-4b99de2daafe"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "review       0\n",
            "sentiment    0\n",
            "dtype: int64\n",
            "Logistic Regression Accuracy: 0.8951\n",
            "✅ Models aur vectorizer successfully save ho gaye Google Drive me: /content/drive/MyDrive/MovieSentModels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 2️⃣ Ensure models folder exists\n",
        "if not os.path.exists(\"models\"):\n",
        "    os.makedirs(\"models\")\n",
        "\n",
        "# 3️⃣ Load dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/IMDB Dataset.csv\")  # Update path if needed\n",
        "df = df.dropna()\n",
        "df = df.drop_duplicates()\n",
        "df['review'] = df['review'].astype(str)\n",
        "df['sentiment'] = df['sentiment'].str.lower()\n",
        "\n",
        "# 4️⃣ Encode labels\n",
        "le = LabelEncoder()\n",
        "df['label'] = le.fit_transform(df['sentiment'])  # positive=1, negative=0\n",
        "num_classes = len(le.classes_)\n",
        "\n",
        "# 5️⃣ Text Cleaning Function\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)  # remove HTML tags\n",
        "    text = re.sub(r\"[^a-zA-Z ]\", \"\", text)  # remove special chars & numbers\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "df['clean_review'] = df['review'].apply(clean_text)\n",
        "\n",
        "# 6️⃣ Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['clean_review'], df['label'], test_size=0.3, stratify=df['label'], random_state=42\n",
        ")\n",
        "\n",
        "# 7️⃣ Tokenization & Padding for LSTM\n",
        "max_words = 10000\n",
        "max_len = 200\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "# 8️⃣ Save Tokenizer\n",
        "with open(\"models/tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "# 9️⃣ LSTM Model\n",
        "embedding_dim = 100\n",
        "\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
        "lstm_model.add(LSTM(128, return_sequences=False))\n",
        "lstm_model.add(Dropout(0.5))\n",
        "lstm_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 10️⃣ Train LSTM\n",
        "history = lstm_model.fit(\n",
        "    X_train_pad, y_train,\n",
        "    validation_data=(X_test_pad, y_test),\n",
        "    epochs=10,  # Colab fast test; increase to 10-15 for better results\n",
        "    batch_size=64\n",
        ")\n",
        "# LSTM model save karna\n",
        "lstm_model.save(os.path.join(drive_model_path, \"lstm_model.h5\"))\n",
        "\n",
        "# Tokenizer save karna\n",
        "import pickle\n",
        "\n",
        "with open(os.path.join(drive_model_path, \"tokenizer.pkl\"), \"wb\") as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(f\"✅ LSTM model aur tokenizer bhi successfully save ho gaye Google Drive me: {drive_model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p56cExSwg7x",
        "outputId": "acf9db46-cd03-44e9-ee13-b1cf54b75628"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 491ms/step - accuracy: 0.5180 - loss: 0.6930 - val_accuracy: 0.5059 - val_loss: 0.6856\n",
            "Epoch 2/10\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 509ms/step - accuracy: 0.6079 - loss: 0.6573 - val_accuracy: 0.6153 - val_loss: 0.6436\n",
            "Epoch 3/10\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 506ms/step - accuracy: 0.7076 - loss: 0.5902 - val_accuracy: 0.5776 - val_loss: 0.6700\n",
            "Epoch 4/10\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 493ms/step - accuracy: 0.7159 - loss: 0.5459 - val_accuracy: 0.8615 - val_loss: 0.3309\n",
            "Epoch 5/10\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 492ms/step - accuracy: 0.8999 - loss: 0.2604 - val_accuracy: 0.8690 - val_loss: 0.3191\n",
            "Epoch 6/10\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 509ms/step - accuracy: 0.9325 - loss: 0.1915 - val_accuracy: 0.8633 - val_loss: 0.3451\n",
            "Epoch 7/10\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 508ms/step - accuracy: 0.9567 - loss: 0.1336 - val_accuracy: 0.8636 - val_loss: 0.3551\n",
            "Epoch 8/10\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 509ms/step - accuracy: 0.9715 - loss: 0.0949 - val_accuracy: 0.8624 - val_loss: 0.4528\n",
            "Epoch 9/10\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 499ms/step - accuracy: 0.9813 - loss: 0.0699 - val_accuracy: 0.8606 - val_loss: 0.5274\n",
            "Epoch 10/10\n",
            "\u001b[1m543/543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 508ms/step - accuracy: 0.9874 - loss: 0.0501 - val_accuracy: 0.8621 - val_loss: 0.6119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LSTM model aur tokenizer bhi successfully save ho gaye Google Drive me: /content/drive/MyDrive/MovieSentModels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CwcDZ3xWwhHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xKqjaMv5whK9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}